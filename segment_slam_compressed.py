from typing import Literal
import tyro
import os
import torch
import cv2
import imageio  # To generate gifs
import pycolmap_scene_manager as pycolmap
from gsplat import rasterization
import numpy as np
import clip
import matplotlib
matplotlib.use("TkAgg")

# from lseg import LSegNet
import json
import sys
import torch.nn as nn

from transformers import CLIPProcessor, CLIPModel

class EncoderDecoder(nn.Module):
    def __init__(self):
        super(EncoderDecoder, self).__init__()
        self.encoder = nn.Parameter(torch.randn(512, 16))
        self.decoder = nn.Parameter(torch.randn(16, 512))

    def forward(self, x):
        x = x @ self.encoder
        y = x @ self.decoder
        return x, y


encoder_decoder = EncoderDecoder().to("cuda")
encoder_decoder.load_state_dict(torch.load("/home/siddharth/siddharth/thesis/my_seg_yolo/enc_dec_model/encoder_decoder.ckpt"))


class_names = {
    0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 
    6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 
    11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 
    16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 
    22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 
    27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 
    32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 
    36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 
    40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 
    46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 
    51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 
    57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 
    62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 
    67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 
    72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 
    77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush', 80: 'others'
}

def torch_to_cv(tensor):
    img_cv = tensor.detach().cpu().numpy()[..., ::-1]
    img_cv = np.clip(img_cv * 255, 0, 255).astype(np.uint8)
    return img_cv


def _detach_tensors_from_dict(d, inplace=True):
    if not inplace:
        d = d.copy()
    for key in d:
        if isinstance(d[key], torch.Tensor):
            d[key] = d[key].detach()
    return d

def to_tensor_safe(data):
    if isinstance(data, torch.Tensor):
        return data.clone().detach().float()
    return torch.tensor(data).float()

def splats_to_tensor(splats):
    splats["means"] = to_tensor_safe(splats["means"])
    splats["rotation"] = to_tensor_safe(splats["rotation"])
    features_dc = to_tensor_safe(splats["features_dc"])
    features_rest = to_tensor_safe(splats["features_rest"])
    splats["opacity"] = to_tensor_safe(splats["opacity"])
    splats["scaling"] = to_tensor_safe(splats["scaling"])
    splats["features_dc"] = features_dc[:,None,:].clone()
    splats["features_rest"] = features_rest.reshape(-1, 15, 3).clone()
    
    return splats

def load_checkpoint(
    checkpoint: str,
    camera_json: str,
    rasterizer: Literal["inria", "gsplat"] = "inria",
    data_factor: int = 1,
):
    
    assert os.path.exists(camera_json), f"Camera JSON file {camera_json} does not exist."
    with open(camera_json, "r") as f:
        cameras = json.load(f)

    model = torch.load(checkpoint, weights_only = False)  # Make sure it is generated by 3DGS original repo
    # sys.exit()
    if rasterizer == "inria":
        print("using inria rasterizer")
        model_params = model
        # print(model_params.keys())
        splats = {  
            "active_sh_degree": 3,
            "means": model_params["means"],
            "features_dc": model_params["features_dc"],
            "features_rest": model_params["features_rest"],
            "scaling": model_params["scaling"],
            "rotation": model_params["rotation"],
            "opacity": model_params["opacity"].squeeze(1),
        }
    elif rasterizer == "gsplat":
        print("using gsplat rasterizer")
        print(model["splats"].keys())
        model_params = model["splats"]
        splats = {
            "active_sh_degree": 3,
            "means": model_params["means"],
            "features_dc": model_params["sh0"],
            "features_rest": model_params["shN"],
            "scaling": model_params["scales"],
            "rotation": model_params["quats"],
            "opacity": model_params["opacities"],
        }
    else:
        raise ValueError("Invalid rasterizer")

    _detach_tensors_from_dict(splats)

    first_cam = cameras[0]
    fx,fy = first_cam["fx"], first_cam["fy"]
    cx,cy = first_cam["width"]/2, first_cam["height"]/2

    # Assuming only one camera
    camera_matrix = torch.tensor(
        [
            [fx, 0, cx], 
            [0, fy, cy], 
            [0, 0, 1],
        ]
    )
    camera_matrix[:2, :3] /= data_factor
    
    splats["camera_matrix"] = camera_matrix
    splats["slam_positions"] = [
        {"id":cam["id"], "position": cam["position"],"rotation": cam["rotation"]} 
        for cam in cameras
    ]
    splats["camera_json"] = camera_json
    splats = splats_to_tensor(splats)
    
    return splats

def create_checkerboard(width, height, size=64):
    checkerboard = np.zeros((height, width, 3), dtype=np.uint8)
    for y in range(0, height, size):
        for x in range(0, width, size):
            if (x // size + y // size) % 2 == 0:
                checkerboard[y : y + size, x : x + size] = 255
            else:
                checkerboard[y : y + size, x : x + size] = 128
    return checkerboard

def get_viewmat_position_and_rotation(position,rotation):
    # Convert inputs to tensors
    position = torch.tensor(position, dtype=torch.float)
    rotation = torch.tensor(rotation, dtype=torch.float)
    
    # Construct the view matrix
    viewmat = torch.eye(4, dtype=torch.float32)
    viewmat[:3, :3] = torch.tensor(rotation)
    viewmat[:3, 3] = torch.tensor(position)

    viewmat = torch.inverse(viewmat)
    
    return viewmat

def prune_by_gradients(splats):
    
    means = splats["means"]
    quats = splats["rotation"]
    features_dc = splats["features_dc"]
    features_rest = splats["features_rest"]
    opacities = splats["opacity"]
    scales = splats["scaling"]
    
    colors = torch.cat([features_dc, features_rest], dim=1)

    opacities = torch.sigmoid(opacities)
    scales = torch.exp(scales)
    
    K = splats["camera_matrix"]
    slam_positions = splats["slam_positions"]
    colors.requires_grad = True
    gaussian_grads = torch.zeros(colors.shape[0], device=colors.device)
    
    for cam in slam_positions:
        viewmat = get_viewmat_position_and_rotation(cam["position"], cam["rotation"])
        
        output,_,_=rasterization(
            means,
            quats,
            scales,
            opacities,
            colors[:,0,:],
            viewmats = viewmat[None],
            Ks=K[None],
            width = K[0,2]*2,
            height=K[1,2]*2,
        )
        #Compute pseudo loss and backpropagate
        pseudo_loss = ((output.detach() + 1 - output) ** 2).mean()
        pseudo_loss.backward()
        gaussian_grads += (colors.grad[:, 0]).norm(dim=[1])
        colors.grad.zero_()
    
    mask = gaussian_grads > 0
    print("Total splats", len(gaussian_grads))
    print("Pruned", (~mask).sum().item(), "splats")
    print("Remaining", mask.sum().item(), "splats")
    
    # Apply the mask to prune the splats
    pruned_splats = splats.copy()
    pruned_splats["means"] = means[mask]
    pruned_splats["features_dc"] = features_dc[mask]
    pruned_splats["features_rest"] = features_rest[mask]
    pruned_splats["scaling"] = scales[mask]
    pruned_splats["rotation"] = quats[mask]
    pruned_splats["opacity"] = opacities[mask]
    
    return pruned_splats

def test_proper_pruning(splats, splats_after_pruning):
    # colmap_project = splats["colmap_project"]
    # frame_idx = 0
    means = splats["means"]
    colors_dc = splats["features_dc"]
    colors_rest = splats["features_rest"]
    colors = torch.cat([colors_dc, colors_rest], dim=1)
    opacities = torch.sigmoid(splats["opacity"])
    scales = torch.exp(splats["scaling"])
    quats = splats["rotation"]

    means_pruned = splats_after_pruning["means"]
    colors_dc_pruned = splats_after_pruning["features_dc"]
    colors_rest_pruned = splats_after_pruning["features_rest"]
    colors_pruned = torch.cat([colors_dc_pruned, colors_rest_pruned], dim=1)
    opacities_pruned = torch.sigmoid(splats_after_pruning["opacity"])
    scales_pruned = torch.exp(splats_after_pruning["scaling"])
    quats_pruned = splats_after_pruning["rotation"]

    K = splats["camera_matrix"]
    slam_positions = splats["slam_positions"]
    total_error = 0
    max_pixel_error = 0
    for cam in slam_positions:
        viewmat = get_viewmat_position_and_rotation(cam["position"], cam["rotation"])
        output, _, _ = rasterization(
            means,
            quats,
            scales,
            opacities,
            colors,
            viewmats=viewmat[None],
            Ks=K[None],
            sh_degree=3,
            width=K[0, 2] * 2,
            height=K[1, 2] * 2,
        )

        output_pruned, _, _ = rasterization(
            means_pruned,
            quats_pruned,
            scales_pruned,
            opacities_pruned,
            colors_pruned,
            viewmats=viewmat[None],
            Ks=K[None],
            sh_degree=3,
            width=K[0, 2] * 2,
            height=K[1, 2] * 2,
        )

        total_error += torch.abs((output - output_pruned)).sum()
        max_pixel_error = max(
            max_pixel_error, torch.abs((output - output_pruned)).max()
        )

    percentage_pruned = (
        (len(splats["means"]) - len(splats_after_pruning["means"]))
        / len(splats["means"])
        * 100
    )
    print(
        "Report {}% pruned, max pixel error = {}, total pixel error = {}".format(
            percentage_pruned, max_pixel_error, total_error
        )
    )

def get_mask3d_yolo(splats, gaussian_features, prompt, neg_prompt, threshold=None):
    # Load CLIP model and processor

    clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to("cuda")
    clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    

    # print("gaussian_features_shape",gaussian_features.shape)

    # Process text prompts
    prompts = [prompt] + neg_prompt.split(";")
    inputs = clip_processor(text=prompts, return_tensors="pt", padding=True)
    text_feat = clip_model.get_text_features(**inputs)  # Shape: [num_queries, 512]
    text_feat_norm = torch.nn.functional.normalize(text_feat, p=2, dim=1)
    
    # Compress text prompts 512->16
    text_feat_compressed = text_feat_norm@encoder_decoder.encoder # 512 -> 16
    text_feat = torch.nn.functional.normalize(text_feat_compressed,p=2,dim=1)

    # Compute similarity scores
    score = gaussian_features @ text_feat.T
    
    # Compute masks
    mask_3d = score[:, 0] > score[:, 1:].max(dim=1)[0]
    # print("score shape",score.shape)

    if threshold is not None:
        mask_3d = mask_3d & (score[:, 0] > threshold)
    
    mask_3d_inv = ~mask_3d
    
    return mask_3d, mask_3d_inv

from PIL import Image
def save_mask_from_frame(frame, file_name="output_mask.png", threshold=0.5):
    grayscale = np.mean(frame, axis=-1).astype(np.uint8)
    mask = (grayscale).astype(np.uint8)
    mask_image = Image.fromarray(mask, mode="L")
    mask_image.save(file_name)
    print(f"2D mask saved as {file_name}")

def get_2d_mask(splats, test_images, no_sh=True):
    means = splats["means"]
    colors_dc = splats["features_dc"]
    colors_rest = splats["features_rest"]
    colors = torch.cat([colors_dc, colors_rest], dim=1)
    if no_sh == True:
        colors = colors_dc[:, 0, :]
    opacities = torch.sigmoid(splats["opacity"])
    scales = torch.exp(splats["scaling"])
    quats = splats["rotation"]
    K = splats["camera_matrix"]

    
    for cam in splats["slam_positions"]:    
        if cam['id'] not in test_images:
            print(f"Skipping {cam['id']} as it is train image")
            continue

        viewmat = get_viewmat_position_and_rotation(cam["position"], cam["rotation"])
        output, alphas, meta = rasterization(
            means,
            quats,
            scales,
            opacities,
            colors,
            viewmat[None],
            K[None],
            width=K[0, 2] * 2,
            height=K[1, 2] * 2,
            sh_degree=3 if not no_sh else None,
        )
        frame = np.clip(output[0].detach().cpu().numpy() * 255, 0, 255).astype(np.uint8)
        print(cam.keys())
        image_name = cam["id"]
        print(image_name)

        # frame = np.clip(output[0].detach().cpu().numpy() * 255, 0, 255).astype(np.uint8)

        # Save the 2D mask based on the output image
        save_mask_from_frame(frame, f"{image_name}.jpg")
        # save_mask_from_alphas(alphas[0], f"{image.name}")



def apply_mask3d(splats, mask3d, mask3d_inverted):
    if mask3d_inverted == None:
        mask3d_inverted = ~mask3d
    extracted = splats.copy()
    deleted = splats.copy()
    masked = splats.copy()
    extracted["means"] = extracted["means"][mask3d]
    extracted["features_dc"] = extracted["features_dc"][mask3d]
    extracted["features_rest"] = extracted["features_rest"][mask3d]
    extracted["scaling"] = extracted["scaling"][mask3d]
    extracted["rotation"] = extracted["rotation"][mask3d]
    extracted["opacity"] = extracted["opacity"][mask3d]

    deleted["means"] = deleted["means"][mask3d_inverted]
    deleted["features_dc"] = deleted["features_dc"][mask3d_inverted]
    deleted["features_rest"] = deleted["features_rest"][mask3d_inverted]
    deleted["scaling"] = deleted["scaling"][mask3d_inverted]
    deleted["rotation"] = deleted["rotation"][mask3d_inverted]
    deleted["opacity"] = deleted["opacity"][mask3d_inverted]

    masked["features_dc"][mask3d] = 1  # (1 - 0.5) / 0.2820947917738781
    masked["features_dc"][~mask3d] = 0  # (0 - 0.5) / 0.2820947917738781
    masked["features_rest"][~mask3d] = 0

    return extracted, deleted, masked


def render_to_gif(
    output_path: str,
    splats,
    feedback: bool = False,
    use_checkerboard_background: bool = False,
    use_white_background: bool = False,
    no_sh: bool = False,
):
    if feedback:
        cv2.destroyAllWindows()
        cv2.namedWindow("Rendering", cv2.WINDOW_NORMAL)
    frames = []
    means = splats["means"]
    colors_dc = splats["features_dc"]
    colors_rest = splats["features_rest"]
    colors = torch.cat([colors_dc, colors_rest], dim=1)
    if no_sh == True:
        colors = colors_dc[:, 0, :]
    opacities = torch.sigmoid(splats["opacity"])
    scales = torch.exp(splats["scaling"])
    quats = splats["rotation"]
    K = splats["camera_matrix"]
    aux_dir = output_path + ".images"

    os.makedirs(aux_dir, exist_ok=True)

    for cam in splats["slam_positions"]:    
        viewmat = get_viewmat_position_and_rotation(cam["position"], cam["rotation"])
        output, alphas, meta = rasterization(
            means,
            quats,
            scales,
            opacities,
            colors,
            viewmat[None],
            K[None],
            width=K[0, 2] * 2,
            height=K[1, 2] * 2,
            sh_degree=3 if not no_sh else None,
        )
        frame = np.clip(output[0].detach().cpu().numpy() * 255, 0, 255).astype(np.uint8)
        if use_checkerboard_background:
            checkerboard = create_checkerboard(frame.shape[1], frame.shape[0])
            alphas = alphas[0].detach().cpu().numpy()
            frame = frame * alphas + checkerboard * (1 - alphas)
            frame = np.clip(frame, 0, 255).astype(np.uint8)
            
            
        if use_white_background:
            white_bg = np.ones_like(frame) * 255  # Create a white background
            alphas = alphas[0].detach().cpu().numpy()
            frame = frame * alphas + white_bg * (1 - alphas)
            frame = np.clip(frame, 0, 255).astype(np.uint8)
            
        frames.append(frame)
        if feedback:
            cv2.imshow("Rendering", frame[..., ::-1])
            cv2.imwrite(f"{aux_dir}/{cam['id']}.jpg", frame[..., ::-1])
            cv2.waitKey(1)
    imageio.mimsave(output_path, frames, fps=30, loop=0)
    if feedback:
        cv2.destroyAllWindows()

def main(
    # json_directory: str = "/home/siddharth/siddharth/thesis/RTG-SLAM/output/dataset/Replica/office0/cameras.json",  # camera json file
    # checkpoint: str = "/home/siddharth/siddharth/thesis/RTG-SLAM/output/dataset/Replica/office0/save_model/frame_2000/iter_1139_stable.pth",  # checkpoint path, can generate from original 3DGS repo
    # results_dir: str = "/home/siddharth/siddharth/thesis/my_seg/results/replica/office0",  # output path
    
    json_directory: str = "/home/siddharth/siddharth/thesis/RTG-SLAM/output/dataset/Replica/office0/cameras.json",  # camera json file
    checkpoint: str = "/home/siddharth/siddharth/thesis/RTG-SLAM/output/dataset/Replica/office0/save_model/frame_2000/iter_1139_stable.pth",  # checkpoint path, can generate from original 3DGS repo
    results_dir: str = "/home/siddharth/siddharth/thesis/Yolo_segmentation/results/replica/office0",  # output path
    
    rasterizer: Literal[
        "inria", "gsplat"
    ] = "inria",  # Original or gsplat for checkpoints
    prompt: str = "chair", # the one to be extracted or deleted
    data_factor: int = 4,
    show_visual_feedback: bool = True,

):
    

    test_images = [0,78,1488]
    
    # test_images = {"frame_000643.jpg", "frame_000644.jpg","frame_000645.jpg","frame_000646.jpg","test_1.jpg", "test_2.jpg", "test_3.jpg", "frame_00131.jpg"} 
    

    # Compute negative classes dynamically
    if prompt in class_names.values():
        neg_classes = [name for name in class_names.values() if name != prompt] + ["others"]
    else:
        neg_classes = ["others"]

    neg_prompt = "; ".join(neg_classes)
    print(neg_prompt)
    # sys.exit()
    if not torch.cuda.is_available():
        raise RuntimeError("CUDA is required for this demo")

    torch.set_default_device("cuda")

    os.makedirs(results_dir, exist_ok=True)
    splats = load_checkpoint(
        checkpoint, json_directory, rasterizer=rasterizer, data_factor=data_factor
    )

    # splats_optimized = prune_by_gradients(splats)
    # test_proper_pruning(splats, splats_optimized)
    # splats = splats_optimized

    features = torch.load(f"{results_dir}/features.pt")
    mask3d, mask3d_inv = get_mask3d_yolo(splats, features, prompt, neg_prompt)
    # # To debug
    # mask3d[mask3d==False] = True
    extracted, deleted, masked = apply_mask3d(splats, mask3d, mask3d_inv)
    get_2d_mask(masked, test_images)
    
    # render_to_gif(
    #     f"{results_dir}/extracted.gif",
    #     extracted,
    #     show_visual_feedback,
    #     use_checkerboard_background=False,
    #     use_white_background=False
    # )
    # render_to_gif(f"{results_dir}/deleted.gif", deleted, show_visual_feedback)


if __name__ == "__main__":
    tyro.cli(main)
